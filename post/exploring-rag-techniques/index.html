<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script data-next-head="">
              (function() {
                try {
                  const theme = document.cookie.split('; ').find(row => row.startsWith('theme='))?.split('=')[1];
                  if (theme === 'dark') {
                    document.body.className = 'dark-theme';
                  } else {
                    document.body.className = 'light-theme';
                  }
                } catch (e) {
                  document.body.className = 'light-theme';
                }
              })();
            </script><title data-next-head="">Exploring RAG (Retrieval-Augmented Generation) Techniques with Example Code<!-- --> - The Tech Oracle by Elijah Mondero</title><meta name="description" content="Discover the power of Retrieval-Augmented Generation (RAG) techniques in enhancing the capabilities of language models. This blog post provides a detailed guide on building a RAG application with LangChain, including example code and practical use cases." data-next-head=""/><meta name="keywords" content="RAG, Retrieval-Augmented Generation, LangChain, Example Code, AI Techniques" data-next-head=""/><link rel="icon" href="/favicon.ico" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-P05ETFHS5F"></script><script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){dataLayer.push(arguments);}
                gtag('js', new Date());
                gtag('config', 'G-P05ETFHS5F', {
                  page_path: window.location.pathname,
                });
              </script><link rel="preload" href="/_next/static/css/c50a86251e703078.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c50a86251e703078.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-5f2c93b329773145.js" defer=""></script><script src="/_next/static/chunks/framework-16252ba0501bace7.js" defer=""></script><script src="/_next/static/chunks/main-cae0f42fdc1bda7e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8d3d088556ce620d.js" defer=""></script><script src="/_next/static/chunks/230-8a10a6030a242aaa.js" defer=""></script><script src="/_next/static/chunks/891-f810e33840a1d596.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bid%5D-1bdc5ace548c1428.js" defer=""></script><script src="/_next/static/BU0zuOZQ6iGF_-Uj6RtaG/_buildManifest.js" defer=""></script><script src="/_next/static/BU0zuOZQ6iGF_-Uj6RtaG/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="App"><div class="reading-progress"><div class="reading-progress-bar" style="width:0%"></div></div><header class="App-header"><h1><a class="home-link" href="/">The Tech Oracle</a></h1><label class="switch"><input type="checkbox"/><span class="slider"></span></label></header><nav class="breadcrumb"><a class="breadcrumb-link" href="/">Home</a><span class="breadcrumb-separator"> › </span><a class="breadcrumb-link" href="/">Posts</a><span class="breadcrumb-separator"> › </span><span class="breadcrumb-current">Exploring RAG (Retrieval-Augmented Generation) Techniques with Example Code</span></nav><div class="blog-layout"><article class="blog-post-content"><header class="article-header"><div class="article-image-container"><img src="/posts/images/36d0755b-6e08-4ae1-98d3-397ff1ea5678.png" alt="Exploring RAG (Retrieval-Augmented Generation) Techniques with Example Code" class="article-hero-image"/></div><div class="article-metadata"><div class="article-category"><span class="category-tag">AI &amp; Technology</span></div><h1 class="article-title">Exploring RAG (Retrieval-Augmented Generation) Techniques with Example Code</h1><p class="article-summary">Discover the power of Retrieval-Augmented Generation (RAG) techniques in enhancing the capabilities of language models. This blog post provides a detailed guide on building a RAG application with LangChain, including example code and practical use cases.</p><div class="article-meta-info"><div class="author-info"><span class="author-name">By <!-- -->Elijah Mondero</span></div><div class="article-stats"><span class="publish-date">February 21, 2025</span><span class="reading-time">2<!-- --> min read</span></div></div><div class="social-share"><span class="share-label">Share:</span><a href="https://twitter.com/intent/tweet?text=Exploring%20RAG%20(Retrieval-Augmented%20Generation)%20Techniques%20with%20Example%20Code&amp;url=" target="_blank" rel="noopener noreferrer" class="share-button twitter">Twitter</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=" target="_blank" rel="noopener noreferrer" class="share-button linkedin">LinkedIn</a><button class="share-button copy">Copy Link</button></div></div></header><div class="article-content"><div><p>Retrieval-Augmented Generation (RAG) is a powerful technique that combines retrieval-based methods with generative models to improve the accuracy and relevance of generated content. By leveraging large datasets, RAG can answer questions more effectively by retrieving pertinent information and generating contextually appropriate responses. In this blog post, we will explore the steps to build a RAG application using LangChain and provide example code to demonstrate its capabilities.</p>
<h3>Prerequisites</h3>
<p>Before diving into the implementation, ensure you have the required libraries installed. Execute the following command to install the necessary packages:</p>
<pre><code class="language-bash">!pip install langchain langchain_community langchainhub langchain-openai tiktoken chromadb
</code></pre>
<h3>Setting Up Environment Variables</h3>
<p>LangChain integrates with various APIs to enable tracing and embedding generation. Set up the required environment variables for LangChain and OpenAI:</p>
<pre><code class="language-python">import os
os.environ['LANGSMITH_TRACING'] = 'true'
os.environ['LANGSMITH_API_KEY'] = '&#x3C;langsmith-api-key>'
os.environ['OPENAI_API_KEY'] = '&#x3C;openai-api-key>'
</code></pre>
<h3>Step 1: Indexing Content</h3>
<p>Indexing is the process of preparing your dataset for retrieval. In this example, we load and process a blog post for indexing.</p>
<h4>Loading the Blog Content</h4>
<p>We use <code>WebBaseLoader</code> to scrape the content from a blog URL. In this case, the content is restricted to certain HTML classes using BeautifulSoup:</p>
<pre><code class="language-python">import bs4
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/\",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=(\"post-content\", \"post-title\", \"post-header\")
        )
    ),
)
blog_docs = loader.load()
</code></pre>
<h4>Splitting the Content</h4>
<p>Large documents need to be divided into manageable chunks for efficient retrieval. This process ensures that the system can handle queries effectively by focusing on smaller, relevant sections of data:</p>
<pre><code class="language-python">from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=300,
    chunk_overlap=50
)
splits = text_splitter.split_documents(blog_docs)
</code></pre>
<h4>Indexing with Embeddings</h4>
<p>The document chunks are converted into vector embeddings using OpenAI’s embedding model and stored in a vector database (Chroma):</p>
<pre><code class="language-python">from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

embedding = OpenAIEmbeddings(model="text-embedding-3-large")
vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)
retriever = vectorstore.as_retriever()
</code></pre>
<h3>Step 2: Retrieval</h3>
<p>The retriever enables the search functionality for fetching the most relevant chunks of content based on a query:</p>
<pre><code class="language-python">retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
</code></pre>
<h3>Step 3: Generating Responses</h3>
<p>With the retriever in place, we now configure a language model to generate responses based on the retrieved content.</p>
<h3>Conclusion</h3>
<p>RAG techniques significantly enhance the capabilities of language models by combining retrieval-based methods with generative models. By following the steps outlined in this blog post, you can build a RAG application using LangChain and explore the potential of this powerful technique.</p>
</div></div><footer class="article-footer"><div class="article-tags"><span class="tags-label">Tags:</span><span class="tag">RAG</span><span class="tag">Retrieval-Augmented Generation</span><span class="tag">LangChain</span><span class="tag">Example Code</span><span class="tag">AI Techniques</span></div></footer><section class="comments-section"><h3>Comments &amp; Discussion</h3><div class="giscus-container"></div><div class="comments-fallback"><p>Comments powered by GitHub Discussions. If comments don&#x27;t load, please ensure:</p><ul><li>GitHub Discussions is enabled on the repository</li><li>You&#x27;re signed in to GitHub</li><li>JavaScript is enabled in your browser</li></ul><p>You can also comment directly on<!-- --> <a href="https://github.com/elijahmondero/elijahmondero.github.io/discussions" target="_blank" rel="noopener noreferrer" class="github-discussions-link">GitHub Discussions</a></p></div></section></article></div><button class="back-to-top" style="opacity:0">↑ Top</button></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"id":"exploring-rag-techniques","title":"Exploring RAG (Retrieval-Augmented Generation) Techniques with Example Code","date":"2025-02-21T06:33:24.997916Z","contentHtml":"\u003cp\u003eRetrieval-Augmented Generation (RAG) is a powerful technique that combines retrieval-based methods with generative models to improve the accuracy and relevance of generated content. By leveraging large datasets, RAG can answer questions more effectively by retrieving pertinent information and generating contextually appropriate responses. In this blog post, we will explore the steps to build a RAG application using LangChain and provide example code to demonstrate its capabilities.\u003c/p\u003e\n\u003ch3\u003ePrerequisites\u003c/h3\u003e\n\u003cp\u003eBefore diving into the implementation, ensure you have the required libraries installed. Execute the following command to install the necessary packages:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e!pip install langchain langchain_community langchainhub langchain-openai tiktoken chromadb\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eSetting Up Environment Variables\u003c/h3\u003e\n\u003cp\u003eLangChain integrates with various APIs to enable tracing and embedding generation. Set up the required environment variables for LangChain and OpenAI:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport os\nos.environ['LANGSMITH_TRACING'] = 'true'\nos.environ['LANGSMITH_API_KEY'] = '\u0026#x3C;langsmith-api-key\u003e'\nos.environ['OPENAI_API_KEY'] = '\u0026#x3C;openai-api-key\u003e'\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eStep 1: Indexing Content\u003c/h3\u003e\n\u003cp\u003eIndexing is the process of preparing your dataset for retrieval. In this example, we load and process a blog post for indexing.\u003c/p\u003e\n\u003ch4\u003eLoading the Blog Content\u003c/h4\u003e\n\u003cp\u003eWe use \u003ccode\u003eWebBaseLoader\u003c/code\u003e to scrape the content from a blog URL. In this case, the content is restricted to certain HTML classes using BeautifulSoup:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\\\"post-content\\\", \\\"post-title\\\", \\\"post-header\\\")\n        )\n    ),\n)\nblog_docs = loader.load()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eSplitting the Content\u003c/h4\u003e\n\u003cp\u003eLarge documents need to be divided into manageable chunks for efficient retrieval. This process ensures that the system can handle queries effectively by focusing on smaller, relevant sections of data:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=300,\n    chunk_overlap=50\n)\nsplits = text_splitter.split_documents(blog_docs)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eIndexing with Embeddings\u003c/h4\u003e\n\u003cp\u003eThe document chunks are converted into vector embeddings using OpenAI’s embedding model and stored in a vector database (Chroma):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003efrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\nembedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\nvectorstore = Chroma.from_documents(documents=splits, embedding=embedding)\nretriever = vectorstore.as_retriever()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eStep 2: Retrieval\u003c/h3\u003e\n\u003cp\u003eThe retriever enables the search functionality for fetching the most relevant chunks of content based on a query:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eStep 3: Generating Responses\u003c/h3\u003e\n\u003cp\u003eWith the retriever in place, we now configure a language model to generate responses based on the retrieved content.\u003c/p\u003e\n\u003ch3\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eRAG techniques significantly enhance the capabilities of language models by combining retrieval-based methods with generative models. By following the steps outlined in this blog post, you can build a RAG application using LangChain and explore the potential of this powerful technique.\u003c/p\u003e\n","excerpt":"Discover the power of Retrieval-Augmented Generation (RAG) techniques in enhancing the capabilities of language models. This blog post provides a detailed guide on building a RAG application with LangChain, including example code and practical use cases.","postedBy":"Elijah Mondero","tags":["RAG","Retrieval-Augmented Generation","LangChain","Example Code","AI Techniques"],"image_path":"/posts/images/36d0755b-6e08-4ae1-98d3-397ff1ea5678.png"}},"__N_SSG":true},"page":"/post/[id]","query":{"id":"exploring-rag-techniques"},"buildId":"BU0zuOZQ6iGF_-Uj6RtaG","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>